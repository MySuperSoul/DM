### Assignment3

---

#### 1. Neural Networks

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**（1）fullyconnect_feedforward**: In this section we need to implement the feedforward for fullyconnect layer. The input parameters is `in, weight, bias`, this process is simply multiply the `in` and `weight`, and remember to add the bias for every image, so the implementing code is below, also can see in `fullyconnect_feedforward.m`:
```matlab
function [out] = fullyconnect_feedforward(in,  weight, bias)
%The feedward process of fullyconnect
%   input parameters:
%       in      : the intputs, shape: [number of images, number of inputs]
%       weight  : the weight matrix, 
%                 shape: [number of inputs, number of outputs]
%       bias    : the bias, shape: [number of outputs, 1]
%
%   output parameters:
%       out     : the output of this layer, 
%                 shape: [number of images, number of outputs]

% TODO
out = in * weight;
for i=1:size(out, 1)
    out(i, :) += bias';
end
end
```

<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(2) relu_feedforward:** In this section we need to implement the relu layer. We know that the relu representation is $relu(x) = max(0, x)$, so I simply transform all the value which is minus to 0. The implementing code is below, also can see in `relu_feedforward.m`:
```matlab
function [ out ] = relu_feedforward( in )
%The feedward process of relu
%   inputs:
%           in	: the input, shape: any shape of matrix
%   
%   outputs:
%           out : the output, shape: same as in

% TODO
    tmp = in;
    tmp(tmp < 0) = 0;
    out = tmp;
end
```

<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(3) fullyconnect_backprop:** In this section we need to implement the fullyconnect layer for backprop process. The input is `in_sensitive, in, weight`, the output is `weight_grad, bias_grad, out_sensitivity`, in BP algorithm, $\frac{\partial net_k}{\partial \omega_{kj}} = y_j$, so the `weight_grad` can simply get from `in * in_sensitive / num_of_images`, and since the bias terms is 1, so the `bias_grad` can get from `sum(in_sensitive) / num_of_images`, then for `out_sensitivity`, this is get from multiply the in_sensitive and weight, that is to say `out_sensitivity = in_sensitivity * weight`. So the implementing is below, also can see in `fullyconnect_backprop.m`:
```matlab
number_of_images = size(in_sensitivity, 1);
weight_grad = in' * in_sensitivity / number_of_images;

bias_grad = (sum(in_sensitivity))' / number_of_images;

out_sensitivity = in_sensitivity * weight';
```

<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(4) relu_backprop:** In this section, we know before that $relu(x) = max(0, x)$, so in the BP procedure, the propagation is that:
$$
\begin{cases}
    in < 0 \implies relu_{grad} = 0 \\
    in > 0 \implies relu_{grad} = 1 \\
\end{cases}
$$

Then `out_sensitivity = relu_grad .* in_sensitivity`. The implementing code is below, and also can see in `relu_backprop.m`:
```matlab
function [out_sensitivity] = relu_backprop(in_sensitivity, in)
%The backpropagation process of relu
%   input paramter:
%       in_sensitivity  : the sensitivity from the upper layer, shape: 
%                       : [number of images, number of outputs in feedforward]
%       in              : the input in feedforward process, 
%                         shape: same as in_sensitivity
%   
%   output paramter:
%       out_sensitivity : the sensitivity to the lower layer, 
%                         shape: same as in_sensitivity

% TODO
relu_grad = in;
relu_grad(relu_grad < 0) = 0;
relu_grad(relu_grad > 0) = 1;
out_sensitivity = relu_grad .* in_sensitivity;
end
```

<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(5) Gradient check:** This part I do gradient check in the code folder, and get the result below, which shows that the gradient calculation is correct:<br>
![](../answer_images/hw3_1_gc.png)

<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(6) Results:** This section I will show the result of my implementing algorithm, after running my algorithm, I get that `loss = 0.2487, accuracy = 0.931`. You can simply run the `run.m` and see the result.

<br>

---

#### 2. K-Nearest Neighbor

**(a)**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; First I implement the knn algorithm using professor DengCai's `EuDist2.m` code. First get the distance matrix, and then find the k nearest point using `sort` built-in function, we just need to find the first k index. Then using the `mode` built-in function to find the predicted label here. So the implementing for knn is below, also can see in `knn.m`:
```matlab
function y = knn(X, X_train, y_train, K)
%KNN k-Nearest Neighbors Algorithm.
%
%   INPUT:  X:         testing sample features, P-by-N_test matrix.
%           X_train:   training sample features, P-by-N matrix.
%           y_train:   training sample labels, 1-by-N row vector.
%           K:         the k in k-Nearest Neighbors
%
%   OUTPUT: y    : predicted labels, 1-by-N_test row vector.
%

% YOUR CODE HERE
dist_matrix = EuDist2(X', X_train');
[Y, idx] = sort(dist_matrix, 2);
idx_k = idx(:, 1:K);
if(K == 1)
    y = y_train(idx_k);
else
    y = mode(y_train(idx_k), 2)';
end
end
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; After implementing knn algorithm, I use the `knn_exp.m` to plot the result, shown below:

- k = 1
![](../answer_images/hw3_2_k_1.png)

<br>

- k = 10
![](../answer_images/hw3_2_k_10.png)

<br>

- k = 100
![](../answer_images/hw3_2_k_100.png)

<br>

**(b)**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In my view, denote that `N` is total number of dataset, and `C` is the class number, the proper `K = N / C`, because can get proper information for one point's neighbors information.

<br>

**(c)**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In this section I prepared 20 training samples in `TrainingSet` folder you can find them. And 5 testing samples in `TestSet` folder. The `labeling` process is in `label_data.m` and save the data in `hack_data.mat`. Then implement the code in `hack.m` and set `K = 3`:
```matlab
function digits = hack(img_name)
%HACK Recognize a CAPTCHA image
%   Inputs:
%       img_name: filename of image
%   Outputs:
%       digits: 1x5 matrix, 5 digits in the input CAPTCHA image.

load('hack_data');
% YOUR CODE HERE
Xtest = extract_image(img_name);
digits = knn(double(Xtest), double(X), double(y), 3);
end
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The main processing code is in `run.m`, you can run this program and see the knn's performance in testing set, I run it and get the results below, it can recognize all correctly!

- (1)
![](../answer_images/hw3_2_test_1.png)

- (2)
![](../answer_images/hw3_2_test_2.png)

- (3)
![](../answer_images/hw3_2_test_3.png)

- (4)
![](../answer_images/hw3_2_test_4.png)

- (5)
![](../answer_images/hw3_2_test_5.png)